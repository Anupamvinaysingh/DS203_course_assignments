{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24B2120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E11 - SPARK\n",
    "\n",
    "---\n",
    "\n",
    "This exercise **should be completed using the Azure VM** that you have created\n",
    "  - Transfer this Notebook (***E11.ipynb***) to the VM\n",
    "  - Create the solution using the VM\n",
    "  - After solving the problem transfer the Notebook back to your local computer\n",
    "  - Upload it to the E11 submission point\n",
    "  - Do not forget to answer the questions asked in **https://forms.gle/86FH1Ng8Ywdcf16TA** (use your iitb.ac.in credentials)\n",
    "\n",
    "**Submissions due by: November 26, 23:55 hrs** (strictly no extensions will be given)\n",
    "\n",
    "Post your queries, if any, to the Moodle forum: [**Queries and Discussions**](https://moodle.iitb.ac.in/mod/forum/view.php?id=75889)\n",
    "\n",
    "---\n",
    "\n",
    "### Part-1\n",
    "#### This section illustrates the use of SPARK Dataframe functions to process nsedata.csv\n",
    "- Review **Part-1** to understand the code by referring to SPARK documentation.\n",
    "- Add your comment to each cell, to explain its purpose\n",
    "- Add code / create additional cells for debugging purpose, and comment them too\n",
    "\n",
    "### Part-2 **[10 marks]**\n",
    "\n",
    "  - This part carries 10 marks\n",
    "    - 5 marks for the correctness and quality of ***properly commented*** code\n",
    "    - 5 marks for overall correctness of the results\n",
    "<br><br>\n",
    "- Using SPARK functions, solve the problem stated in **Part-2**\n",
    "  - (***DO NOT USE the createTempView function in your solution!***)\n",
    "  - (***DO NOT USE PANDAS Dataframe in your solution***)\n",
    "\n",
    "---\n",
    "\n",
    "### Submission Guidelines\n",
    "- **Part-1 and Part-2**\n",
    "  - Upload this Notebook, with your solutio, to the E11 submission point.\n",
    "    - **BEFORE UPLOADING, ENSURE THAT YOU REMOVE / TRIM LENGTHY DEBUG OUTPUTS**.\n",
    "    - Short debug outputs of up to 5 lines are acceptable.\n",
    "- **Part-2 [10 marks]**\n",
    "  - Answer the questions in the Form: **https://forms.gle/86FH1Ng8Ywdcf16TA**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Part 1</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the findspark module so that PySpark can be imported and used\n",
    "# This allows the notebook to correctly locate and load the Spark installation\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the main PySpark package and required modules\n",
    "import pyspark\n",
    "\n",
    "# Import data type classes used to define a structured schema for reading the CSV file\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Import PySpark SQL functions (aliased as F) used for column operations,\n",
    "# filtering, aggregation, transformations, and date/number manipulation\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/26 16:31:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create the SparkContext, which is the entry point for using Spark's core functionalities.\n",
    "# The 'appName' sets the name of the Spark application visible in logs/UI.\n",
    "sc = pyspark.SparkContext(appName=\"E9-2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SparkSession using the existing SparkContext.\n",
    "# SparkSession is the main entry point for working with DataFrames and SQL operations in PySpark.\n",
    "ss = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reference to the DataFrame reader associated with the SparkSession.\n",
    "# This 'dfr' object will be used later to read data files (e.g., CSV) into Spark DataFrames.\n",
    "dfr = ss.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('SYMBOL', StringType(), True), StructField('SERIES', StringType(), True), StructField('OPEN', DoubleType(), True), StructField('HIGH', DoubleType(), True), StructField('LOW', DoubleType(), True), StructField('CLOSE', DoubleType(), True), StructField('LAST', DoubleType(), True), StructField('PREVCLOSE', DoubleType(), True), StructField('TOTTRDQTY', LongType(), True), StructField('TOTTRDVAL', DoubleType(), True), StructField('TIMESTAMP', StringType(), True), StructField('ADDNL', StringType(), True)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a structured schema for reading the NSE dataset.\n",
    "# This ensures each column is assigned the correct data type when loading the CSV file.\n",
    "\n",
    "schemaStruct = StructType()\n",
    "\n",
    "# Add each column to the schema with its corresponding data type.\n",
    "# The third argument 'True' indicates that the column can contain null values.\n",
    "schemaStruct.add(\"SYMBOL\", StringType(), True)       # Stock symbol\n",
    "schemaStruct.add(\"SERIES\", StringType(), True)       # Series type (e.g., EQ)\n",
    "schemaStruct.add(\"OPEN\", DoubleType(), True)         # Opening price\n",
    "schemaStruct.add(\"HIGH\", DoubleType(), True)         # Highest price of the day\n",
    "schemaStruct.add(\"LOW\", DoubleType(), True)          # Lowest price of the day\n",
    "schemaStruct.add(\"CLOSE\", DoubleType(), True)        # Closing price\n",
    "schemaStruct.add(\"LAST\", DoubleType(), True)         # Last traded price\n",
    "schemaStruct.add(\"PREVCLOSE\", DoubleType(), True)    # Previous day's closing price\n",
    "schemaStruct.add(\"TOTTRDQTY\", LongType(), True)      # Total traded quantity\n",
    "schemaStruct.add(\"TOTTRDVAL\", DoubleType(), True)    # Total traded value\n",
    "schemaStruct.add(\"TIMESTAMP\", StringType(), True)    # Date in string format\n",
    "schemaStruct.add(\"ADDNL\", StringType(), True)        # Additional field (unused)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the NSE dataset CSV file into a Spark DataFrame using the predefined schema.\n",
    "# 'header=True' tells Spark that the first row of the CSV contains column names.\n",
    "df = dfr.csv(\"/home/hduser/spark/nsedata.csv\", schema=schemaStruct, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Basics : Using SPARK for analysis</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a subset DataFrame for a given company (symbol).\n",
    "# It filters rows for the specified company and renames the OHLC columns\n",
    "# so that each company has uniquely identifiable column names when joined later.\n",
    "\n",
    "def create_subset_from_df(company_code):\n",
    "    tcode = company_code.lower()   # Convert company code to lowercase for naming consistency\n",
    "\n",
    "    # Select the OHLC columns and timestamp, renaming them with the company code suffix.\n",
    "    df_subset = df.select(\n",
    "                    F.col(\"OPEN\").alias(\"OPEN_\" + tcode),\n",
    "                    F.col(\"HIGH\").alias(\"HIGH_\" + tcode),\n",
    "                    F.col(\"LOW\").alias(\"LOW_\" + tcode),\n",
    "                    F.col(\"CLOSE\").alias(\"CLOSE_\" + tcode),\n",
    "                    F.col(\"TIMESTAMP\")\n",
    "                ).where(F.col(\"SYMBOL\") == company_code)   # Filter rows belonging to the selected company\n",
    "\n",
    "    return df_subset   # Return the subset DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why do we need to use the alias function, above? What happens if we do not alias / rename the columns?\n",
    "# -----------------------------\n",
    "# When creating a subset for each company, all subsets contain the same column names:\n",
    "# OPEN, HIGH, LOW, CLOSE, TIMESTAMP.\n",
    "#\n",
    "# Later, when multiple company DataFrames are joined together, Spark will face\n",
    "# column name conflicts because the columns are identical across companies.\n",
    "#\n",
    "# If we do NOT rename (alias) the columns:\n",
    "#   - Spark may throw an \"ambiguous column\" error during joins, OR\n",
    "#   - Spark may overwrite columns or pick one arbitrarily, leading to incorrect results.\n",
    "#\n",
    "# By using alias(), we give each column a unique name such as:\n",
    "#   OPEN_reliance, HIGH_reliance, CLOSE_tcs, LOW_infosys, etc.\n",
    "#\n",
    "# This ensures:\n",
    "#   ✔ No column name conflicts during joins\n",
    "#   ✔ Each company’s data remains clearly separated\n",
    "#   ✔ Results are correct and unambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+----------+-----------+\n",
      "|OPEN_infy|HIGH_infy|LOW_infy|CLOSE_infy|  TIMESTAMP|\n",
      "+---------+---------+--------+----------+-----------+\n",
      "|  2910.75|  2953.55| 2910.75|    2944.2|01-APR-2013|\n",
      "|  3283.05|   3325.0|  3275.0|   3313.95|01-APR-2014|\n",
      "|   2198.9|   2199.5|  2157.7|   2173.95|01-APR-2015|\n",
      "|   2780.1|   2823.8|  2780.1|    2815.1|01-AUG-2011|\n",
      "|   2215.0|   2230.4| 2200.75|   2218.55|01-AUG-2012|\n",
      "+---------+---------+--------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 16:32:30 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+-----------+\n",
      "|summary|         OPEN_infy|        HIGH_infy|          LOW_infy|        CLOSE_infy|  TIMESTAMP|\n",
      "+-------+------------------+-----------------+------------------+------------------+-----------+\n",
      "|  count|              1023|             1023|              1023|              1023|       1023|\n",
      "|   mean|2707.2617302052795|2735.799413489735|2679.3007331378312|2708.2897849462406|       NULL|\n",
      "| stddev| 641.7956780031782|647.6486140678146| 639.1258920145511| 643.8471963450963|       NULL|\n",
      "|    min|             941.0|            952.1|            932.65|             937.5|01-APR-2013|\n",
      "|    max|            4387.0|           4402.2|            4343.4|            4365.9|31-OCT-2014|\n",
      "+-------+------------------+-----------------+------------------+------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a subset DataFrame for the company INFY using the helper function defined earlier.\n",
    "# This subset contains only INFY's OHLC values with uniquely renamed columns.\n",
    "df_infy = create_subset_from_df(\"INFY\")\n",
    "\n",
    "# Display the first 5 rows of the INFY subset to verify that filtering and renaming worked correctly.\n",
    "df_infy.show(5)\n",
    "\n",
    "# Generate summary statistics (count, mean, stddev, min, max) for the INFY columns.\n",
    "# This helps check data quality and understand the distribution of values.\n",
    "df_infy.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+---------+-----------+\n",
      "|OPEN_tcs|HIGH_tcs|LOW_tcs|CLOSE_tcs|  TIMESTAMP|\n",
      "+--------+--------+-------+---------+-----------+\n",
      "|  1185.0| 1198.75|1172.55|  1180.15|01-APR-2011|\n",
      "|  1565.0|  1573.7|1551.25|  1556.85|01-APR-2013|\n",
      "|  2145.0|  2185.0| 2144.9|   2176.7|01-APR-2014|\n",
      "|  2558.0|  2563.6|2522.25|  2542.65|01-APR-2015|\n",
      "|  1142.4|  1149.9| 1125.1|  1135.25|01-AUG-2011|\n",
      "+--------+--------+-------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-----------------+-----------+\n",
      "|summary|          OPEN_tcs|          HIGH_tcs|           LOW_tcs|        CLOSE_tcs|  TIMESTAMP|\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------+\n",
      "|  count|              1240|              1240|              1240|             1240|       1240|\n",
      "|   mean|1678.2334677419353|1697.3971370967743|1658.5663306451613|1677.914798387098|       NULL|\n",
      "| stddev| 598.9262836758081| 603.7783546396355| 592.8736960983055|597.8815093696456|       NULL|\n",
      "|    min|             838.0|             847.7|             830.2|            837.3|01-APR-2011|\n",
      "|    max|            2788.0|            2839.7|            2737.0|           2776.0|31-OCT-2014|\n",
      "+-------+------------------+------------------+------------------+-----------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a subset DataFrame for the company TCS using the same helper function.\n",
    "# This extracts only TCS rows and renames the OHLC columns uniquely for TCS.\n",
    "df_tcs = create_subset_from_df(\"TCS\")\n",
    "\n",
    "# Display the first 5 rows to confirm that the filtering and column renaming worked correctly.\n",
    "df_tcs.show(5)\n",
    "\n",
    "# Show summary statistics (count, mean, stddev, min, max) for the TCS subset.\n",
    "# This helps understand basic data characteristics and check for anomalies.\n",
    "df_tcs.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+\n",
      "|  TIMESTAMP|CLOSE_tcs|CLOSE_infy|\n",
      "+-----------+---------+----------+\n",
      "|02-FEB-2012|   1148.0|    2757.0|\n",
      "|04-DEC-2014|  2637.95|    2101.8|\n",
      "|01-AUG-2013|   1815.4|   2974.65|\n",
      "|01-DEC-2014|  2692.95|   4349.85|\n",
      "|08-FEB-2012|  1219.65|   2769.15|\n",
      "+-----------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the TCS and INFY subset DataFrames on the common column 'TIMESTAMP'.\n",
    "# Since both subsets contain trading data indexed by date, using TIMESTAMP\n",
    "# allows us to align the closing prices of both companies on matching days.\n",
    "\n",
    "df_join = df_tcs.join(df_infy, \"TIMESTAMP\") \\\n",
    "                .select(\"TIMESTAMP\", \"CLOSE_tcs\", \"CLOSE_infy\")\n",
    "\n",
    "# Display the first 5 rows of the joined DataFrame to verify that:\n",
    "# 1. The join worked correctly,\n",
    "# 2. Both closing price columns appear side-by-side for the same dates.\n",
    "df_join.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|         PriceDiff|\n",
      "+-------+------------------+\n",
      "|  count|              1025|\n",
      "|   mean|1163.6446341463443|\n",
      "| stddev|  366.989701532277|\n",
      "|    min|150.95000000000027|\n",
      "|    max|            1804.9|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the absolute difference between the closing prices of TCS and INFY for all aligned dates.\n",
    "# F.abs() computes the absolute value of the difference.\n",
    "df_join.select(F.abs(df_join[\"CLOSE_tcs\"] - df_join[\"CLOSE_infy\"]).alias(\"PriceDiff\")).describe().show()\n",
    "# Describe() calculates and displays summary statistics (count, mean, stddev, min, max) \n",
    "# for the newly computed 'PriceDiff' column. \n",
    "# This helps quantify the average daily price spread and volatility between the two stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+\n",
      "|  TIMESTAMP|CLOSE_tcs|CLOSE_infy|\n",
      "+-----------+---------+----------+\n",
      "|12-FEB-2015|  2462.15|    2311.2|\n",
      "|11-FEB-2015|   2459.9|   2284.85|\n",
      "|10-FEB-2015|  2441.15|    2278.3|\n",
      "+-----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the joined DataFrame (df_join) to find rows (trading days) where the absolute \n",
    "# difference between the closing price of TCS and the closing price of INFY is less than 180.\n",
    "df_join.filter(F.abs(df_join[\"CLOSE_tcs\"] - df_join[\"CLOSE_infy\"]) < 180).show()\n",
    "# This operation identifies dates when the two stocks were trading relatively close in price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import specific date and column functions needed for date manipulation.\n",
    "from pyspark.sql.functions import col, date_format, to_date\n",
    "\n",
    "\n",
    "# Create a new DataFrame (df1) by adding a new column named 'TIMESTAMP2'.\n",
    "df1 = df.withColumn(\"TIMESTAMP2\", date_format(to_date(col(\"TIMESTAMP\"), \"dd-MMM-yyyy\"), \"yyyy-MM\"))\n",
    "# Convert the original 'TIMESTAMP' string column (e.g., '01-Jan-2012') into a Date type,\n",
    "# and then format that Date type as a 'yyyy-MM' string (e.g., '2012-01').\n",
    "# This standardized column is essential for subsequent monthly grouping and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SYMBOL: string (nullable = true)\n",
      " |-- SERIES: string (nullable = true)\n",
      " |-- OPEN: double (nullable = true)\n",
      " |-- HIGH: double (nullable = true)\n",
      " |-- LOW: double (nullable = true)\n",
      " |-- CLOSE: double (nullable = true)\n",
      " |-- LAST: double (nullable = true)\n",
      " |-- PREVCLOSE: double (nullable = true)\n",
      " |-- TOTTRDQTY: long (nullable = true)\n",
      " |-- TOTTRDVAL: double (nullable = true)\n",
      " |-- TIMESTAMP: string (nullable = true)\n",
      " |-- ADDNL: string (nullable = true)\n",
      " |-- TIMESTAMP2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the schema of the DataFrame df1 in a tree format.\n",
    "df1.printSchema()\n",
    "# This is a critical debugging step to verify that:\n",
    "# 1. All original columns from the CSV were loaded with the correct data types \n",
    "#    as specified in the schemaStruct (e.g., 'OPEN' is DoubleType, 'TOTTRDQTY' is LongType).\n",
    "# 2. The new column 'TIMESTAMP2' was successfully added and has the expected string data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 16:34:04 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 14, schema size: 12\n",
      "CSV file: file:///home/hduser/spark/nsedata.csv\n",
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+-------+------+------+------+---------+---------+-------------+-----------+-----+----------+\n",
      "|    SYMBOL|SERIES|  OPEN|   HIGH|   LOW| CLOSE|  LAST|PREVCLOSE|TOTTRDQTY|    TOTTRDVAL|  TIMESTAMP|ADDNL|TIMESTAMP2|\n",
      "+----------+------+------+-------+------+------+------+---------+---------+-------------+-----------+-----+----------+\n",
      "| 20MICRONS|    EQ| 37.75|  37.75| 36.35| 37.45|  37.3|    37.15|    38638|    1420968.1|01-APR-2011|    0|   2011-04|\n",
      "|3IINFOTECH|    EQ| 43.75|   45.3| 43.75|  44.9|  44.8|    43.85|  1239690|5.531120435E7|01-APR-2011|    0|   2011-04|\n",
      "|   3MINDIA|    EQ|3374.0|3439.95|3338.0|3397.5|3400.0|   3364.7|      871|   2941547.35|01-APR-2011|    0|   2011-04|\n",
      "|    A2ZMES|    EQ| 281.8| 294.45| 279.8| 289.2| 287.2|    281.3|   140643| 4.02640755E7|01-APR-2011|    0|   2011-04|\n",
      "|AARTIDRUGS|    EQ| 127.0|  132.0|126.55| 131.3| 130.6|    127.6|     2972|     384468.2|01-APR-2011|    0|   2011-04|\n",
      "+----------+------+------+-------+------+------+------+---------+---------+-------------+-----------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display the first 5 rows of the DataFrame df1.\n",
    "df1.show(5)\n",
    "# This is a visual check to confirm that:\n",
    "# 1. The data was loaded correctly into the DataFrame.\n",
    "# 2. The new 'TIMESTAMP2' column (used for monthly aggregation) has been calculated \n",
    "#    and populated with the correct 'yyyy-MM' format (e.g., '2012-01')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark SQL functions (F) for aggregation.\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group the DataFrame df1 by 'SYMBOL' (stock) and 'TIMESTAMP2' (year-month). \n",
    "# This gathers all daily trading records for a specific stock within a specific month.\n",
    "df_t1 = df1.groupBy(\"SYMBOL\", \"TIMESTAMP2\").agg(\n",
    "    F.min(\"OPEN\"),       # Find the minimum opening price during the month\n",
    "    F.max(\"OPEN\"),       # Find the maximum opening price during the month\n",
    "    F.avg(\"OPEN\"),       # Calculate the average opening price for the month\n",
    "    F.stddev(\"OPEN\"),    # Calculate the standard deviation (volatility) of the opening prices\n",
    "    F.count(\"OPEN\")      # Count the number of trading days in the month\n",
    ")\n",
    "# The resulting DataFrame df_t1 contains monthly statistics for each stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+---------+------------------+------------------+-----------+\n",
      "|    SYMBOL|TIMESTAMP2|min(OPEN)|max(OPEN)|         avg(OPEN)|      stddev(OPEN)|count(OPEN)|\n",
      "+----------+----------+---------+---------+------------------+------------------+-----------+\n",
      "|  AREVAT&D|   2011-04|   246.15|   292.95|274.73055555555555|12.609988324957133|         18|\n",
      "| CHEMPLAST|   2011-04|      6.3|     8.25| 7.172222222222222|0.5570991627387202|         18|\n",
      "|FIRSTLEASE|   2011-04|     68.3|   106.05| 93.05277777777778|10.687822540330412|         18|\n",
      "|    FORTIS|   2011-04|    152.0|    163.4|159.50833333333333|2.7349723087102324|         18|\n",
      "| GOLDINFRA|   2011-04|    16.85|    20.15|17.924999999999997|0.7857648952379039|         18|\n",
      "+----------+----------+---------+---------+------------------+------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display the first 5 rows of the aggregated DataFrame df_t1.\n",
    "df_t1.show(5)\n",
    "# This step verifies the result of the previous aggregation operation, confirming that:\n",
    "# 1. The data was correctly grouped by stock symbol and month.\n",
    "# 2. The aggregate statistics (min, max, avg, stddev, count of OPEN) were correctly calculated \n",
    "#    for the beginning rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the monthly statistics DataFrame (df_t1) to order the time-series data correctly.\n",
    "df_t2 = df_t1.sort(\n",
    "    F.asc(\"SYMBOL\"),        # Primary sort: Ascending (alphabetical) order by stock symbol\n",
    "    F.asc(\"TIMESTAMP2\")     # Secondary sort: Ascending (chronological) order by year-month\n",
    ")\n",
    "# This sorting ensures that the monthly statistics for each stock are sequentially ordered \n",
    "# by time, which is necessary for clear analysis and potential windowing functions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+---------+------------------+------------------+-----------+\n",
      "|   SYMBOL|TIMESTAMP2|min(OPEN)|max(OPEN)|         avg(OPEN)|      stddev(OPEN)|count(OPEN)|\n",
      "+---------+----------+---------+---------+------------------+------------------+-----------+\n",
      "|20MICRONS|   2010-08|     51.6|     54.0| 52.81666666666667|0.9266876496425305|          9|\n",
      "|20MICRONS|   2010-09|     54.9|     64.3| 59.11428571428571| 2.514614426564382|         21|\n",
      "|20MICRONS|   2010-10|    55.05|     60.0|57.166666666666664|1.3035848009751156|         21|\n",
      "|20MICRONS|   2010-11|     53.6|    61.75| 55.98809523809524|2.2001650370997603|         21|\n",
      "|20MICRONS|   2010-12|     38.8|     61.0| 45.66590909090909| 5.796599708606606|         22|\n",
      "+---------+----------+---------+---------+------------------+------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display the first 5 rows of the sorted DataFrame df_t2.\n",
    "df_t2.show(5)\n",
    "# This final visualization step in the aggregation process confirms that:\n",
    "# 1. The data is properly sorted by 'SYMBOL' and 'TIMESTAMP2'.\n",
    "# 2. It shows the monthly statistics for the stock(s) that come first alphabetically and chronologically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Uncomment the following statement to generate the output, and analyze it\n",
    "# Write your observations in the next cell\n",
    "\n",
    "df_t2.write.csv(\"monthly_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your observation related to monthly_stats\n",
    "\n",
    "# The 'monthly_stats.csv' file (or directory) contains the monthly aggregated statistics \n",
    "# for every stock symbol ('SYMBOL') found in the original nsedata.csv. \n",
    "# \n",
    "# The data is structured to show, for each stock (grouped by 'SYMBOL'), \n",
    "# the trading activity for every specific month/year ('TIMESTAMP2') chronologically. \n",
    "# \n",
    "# Specifically, for each SYMBOL/TIMESTAMP2 pair, the file provides the:\n",
    "# 1. Minimum opening price (F.min(\"OPEN\"))[cite: 89].\n",
    "# 2. Maximum opening price (F.max(\"OPEN\"))[cite: 89].\n",
    "# 3. Average opening price (F.avg(\"OPEN\"))[cite: 89].\n",
    "# 4. Standard deviation of the opening price (F.stddev(\"OPEN\")), which is a measure of monthly volatility[cite: 89].\n",
    "# 5. Total count of trading days (F.count(\"OPEN\")) in that month[cite: 89].\n",
    "# \n",
    "# The DataFrame was sorted first by 'SYMBOL' and then by 'TIMESTAMP2', ensuring the \n",
    "# data is organized sequentially for time-series analysis[cite: 92]."
   ]
  },
  {
   "attachments": {
    "007da9c0-de98-47e0-ba90-be3728cf39af.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part - 2\n",
    "###  SPARK Based Stock Analysis and Portfolio Management\n",
    "\n",
    "---\n",
    "\n",
    "  - This part ***has*** to be solved using the Azure VM that you have created\n",
    "  - **This part carries 10 marks**\n",
    "    - 5 marks for the correctness and quality of ***properly commented*** code\n",
    "    - 5 marks for overall correctness of the results\n",
    "\n",
    "---\n",
    "\n",
    "#### Problem Statement\n",
    "Based on equity (EQ) data contained in ***nsedata.csv***, you are tasked with the responsibility to identify **5 pairs of stocks** stocks to invest in, based on the strategy and steps outlined below:\n",
    "\n",
    "You have to process the data for the year 2012, and then make investment decisions for the following year - 2013. Assume that you are doing the analysis and selecting the stocks on Jan 1, 2013. \n",
    "\n",
    "1. You are required to draw up an initial list of stocks - ***USE THE FOLLOWING CODE SEGMENT TO CREATE THE INITIAL LIST OF STOCKS***\n",
    "\n",
    "![E11-initial-list-of-stocks.png](attachment:007da9c0-de98-47e0-ba90-be3728cf39af.png)\n",
    "\n",
    "**How many stocks are there in your list at this stage?**\n",
    "\n",
    "2. From among these stocks you have to then select stocks that have shown maximum overall growth between 1/1/2012 and 31/12/2012 (or the nearest trading days). The hope is that they will continue to grow in the future.\n",
    "    1. How many stocks are there in your list at this stage, that is, after dropping stocks that have NOT shown positive growth?\n",
    "1. You should ensure that volatility and negative market movements will not adversely affect your total investment, substantially.\n",
    "    - One way to achieve this involves selecting ***stock pairs that are negatively correlated***, so that if one stock loses value its partner will most likely gain value - thereby reducing the overall negative impact on the portfolio. As all these stocks are high growth stocks, anyway, the expectation is that the portfolio witll grow, overall. **Note: It is possible for certain stocks to be part of multiple pairs**.\n",
    "    - The next step, therefore, is to create pairwise correlation coefficient data between all the stocks in the list at this stage (stocks that are liquid, and shown growth in 2012)\n",
    "    - Sort the stock pairs in ***ascending*** values of their correlation coefficients, so that the ***most negative correlation values are at the top***\n",
    "    1. Which are the first 5 pairs in this list? Select them.\n",
    "1. Purchase 1 unit of each of these stock pairs on the first trading day of 2013. If some stocks are part of multiple pairs ***purchase 1 unit for each presence***.\n",
    "    1. What is the total value of your investment? \n",
    "1. After you have selected the 5 pairs and made the above investments on 1-Jan-2013, you should assess the situation on 31/12/2013.\n",
    "    1. What is the total value of your investment on 31/12/2013\n",
    "    1. By how much has the portfolio value changed? This can be positive or negative\n",
    "    1. What is the percentage change in the portfolio value? \n",
    "    1. The price of which stocks in your portfolio have increased?\n",
    "    1. The price of which stocks in your portfolio have decreased?\n",
    "1. Comparatively, how did the market perform during the same period? This can be assessed as follows:\n",
    "    1. Considering the period 1/1/2012 - 31/12/2012 create a list of top 25 highly traded stocks with high ***percentage growth***. Which are the first 5 stocks in this list - ***sorted in descending order of their growth percentage***?\n",
    "    1. Purchase 1 stock ***each of these 25 stocks*** on 1/1/2013. What is the total value of this portfolio?\n",
    "    1. What is the value of this portfolio on 31/12/2013\n",
    "    1. What is the percentage change?\n",
    "1. Comment on the efficacy of the adopted strategy (i.e., selecting stocks with negative correlations) against the broad market movement.\n",
    "\n",
    "---\n",
    "\n",
    "**Create SPARK code, below, to solve this problem**\n",
    "  - **DO NOT USE** the ***createTempView*** function in your solution!\n",
    "  - **DO NOT USE** *PANDAS Dataframe* in your solution\n",
    "  - ***IMPORTANT**: Access the Form **https://forms.gle/86FH1Ng8Ywdcf16TA** and answer the questions posed therein** (**use your iitb.ac.in credentials**) before the submission deadline\n",
    "  - Upload this Notebook to the E11 submission point before the submission deadline\n",
    "\n",
    "---\n",
    "\n",
    "Post your queries, if any, to the Moodle forum: [**Queries and Discussions**](https://moodle.iitb.ac.in/mod/forum/view.php?id=75889)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Preparation Complete ---\n",
      "\n",
      "--- 1. Initial Liquidity Screening (2012) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 16:59:38 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 14, schema size: 12\n",
      "CSV file: file:///home/hduser/spark/nsedata.csv\n",
      "[Stage 391:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Total number of liquid stocks (500,000 < QTY < 10,000,000): 211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import all required functions and modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, year, col, min, max, first, avg, desc, asc\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from pyspark.mllib.stat import Statistics # For correlation calculation\n",
    "import builtins # To access native Python sum function\n",
    "import sys # For error handling\n",
    "\n",
    "# --- Prerequisites ---\n",
    "# Assumed: SparkSession is initialized, and 'df' (original nsedata DataFrame) is loaded.\n",
    "# Example initialization (if not provided):\n",
    "# spark = SparkSession.builder.appName(\"NSEAnalysis\").getOrCreate()\n",
    "# df = spark.read.csv(\"path/to/nsedata.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# --- Data Preparation: Date and Year Columns ---\n",
    "df = df.withColumn(\"trade_date\", to_date(col(\"TIMESTAMP\"), \"dd-MMM-yyyy\")) \\\n",
    "       .withColumn(\"trade_year\", year(col(\"trade_date\")))\n",
    "df.cache() # Cache the base DataFrame for repeated filtering\n",
    "\n",
    "print(\"--- Data Preparation Complete ---\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. INITIAL LIQUIDITY SCREENING (2012)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 1. Initial Liquidity Screening (2012) ---\")\n",
    "\n",
    "# Filter raw data for Equity (EQ) series and the target year (2012)\n",
    "df_2012_eq = df.filter((col(\"SERIES\") == 'EQ') & (col(\"trade_year\") == 2012))\n",
    "df_2012_eq.cache()\n",
    "\n",
    "# Calculate average traded quantity (liquidity) per symbol and apply filters\n",
    "MIN_QTY = 500000\n",
    "MAX_QTY = 10000000\n",
    "\n",
    "df_avg_qty = df_2012_eq.groupBy(\"SYMBOL\").agg(\n",
    "    avg(\"TOTTRDQTY\").alias(\"Avg_Daily_Qty\")\n",
    ")\n",
    "\n",
    "df_liquid_2012 = df_avg_qty.filter(col(\"Avg_Daily_Qty\") < MAX_QTY) \\\n",
    "                           .filter(col(\"Avg_Daily_Qty\") > MIN_QTY) \\\n",
    "                           .orderBy(col(\"Avg_Daily_Qty\").desc())\n",
    "\n",
    "# Collect the list of liquid symbols\n",
    "liquid_symbols = [row.SYMBOL for row in df_liquid_2012.select(\"SYMBOL\").collect()]\n",
    "print(f\"Q1: Total number of liquid stocks ({MIN_QTY:,} < QTY < {MAX_QTY:,}): {len(liquid_symbols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Positive Growth Screening (2012) ---\n",
      "Q2: Total number of stocks with positive growth in 2012: 153\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 2. POSITIVE GROWTH SCREENING (2012)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 2. Positive Growth Screening (2012) ---\")\n",
    "\n",
    "df_daily_subset = df_2012_eq.filter(col(\"SYMBOL\").isin(liquid_symbols))\n",
    "\n",
    "# Find the start and end trading dates for each stock\n",
    "df_date_range = df_daily_subset.groupBy(\"SYMBOL\").agg(\n",
    "    min(\"trade_date\").alias(\"start_dt\"),\n",
    "    max(\"trade_date\").alias(\"end_dt\")\n",
    ")\n",
    "\n",
    "# Join back to get the CLOSE price at the start date\n",
    "df_price_start = df_date_range.join(\n",
    "    df_daily_subset.select(\"SYMBOL\", \"trade_date\", \"CLOSE\").alias(\"d\"),\n",
    "    (df_date_range.SYMBOL == col(\"d.SYMBOL\")) & (df_date_range.start_dt == col(\"d.trade_date\"))\n",
    ").select(df_date_range.SYMBOL.alias(\"stock\"), col(\"d.CLOSE\").alias(\"start_price\"))\n",
    "\n",
    "# Join back to get the CLOSE price at the end date\n",
    "df_price_end = df_date_range.join(\n",
    "    df_daily_subset.select(\"SYMBOL\", \"trade_date\", \"CLOSE\").alias(\"d\"),\n",
    "    (df_date_range.SYMBOL == col(\"d.SYMBOL\")) & (df_date_range.end_dt == col(\"d.trade_date\"))\n",
    ").select(df_date_range.SYMBOL.alias(\"stock\"), col(\"d.CLOSE\").alias(\"end_price\"))\n",
    "\n",
    "# Calculate Growth Percentage and filter for positive growth\n",
    "df_growth_final = df_price_start.join(df_price_end, on=df_price_start.stock == df_price_end.stock, how='inner') \\\n",
    "    .select(df_price_start.stock.alias(\"SYMBOL\"), col(\"start_price\"), col(\"end_price\")) \\\n",
    "    .withColumn(\"Growth_Pct\", (col(\"end_price\") - col(\"start_price\")) / col(\"start_price\") * 100) \\\n",
    "    .filter(col(\"Growth_Pct\") > 0) \\\n",
    "    .orderBy(col(\"Growth_Pct\").desc())\n",
    "\n",
    "# Collect the list of stocks that showed positive growth\n",
    "growth_symbols = [row.SYMBOL for row in df_growth_final.select(\"SYMBOL\").collect()]\n",
    "print(f\"Q2: Total number of stocks with positive growth in 2012: {len(growth_symbols)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Pairwise Correlation and Top 5 Pairs ---\n",
      "DEBUG: Correlating the full set of 153 stocks.\n",
      "A. Top 5 negatively correlated pairs (Q3):\n",
      "  L&TFH vs SITICABLE (Corr: -0.8600)\n",
      "  DBREALTY vs WWIL (Corr: -0.8265)\n",
      "  SITICABLE vs SUJANATOW (Corr: -0.8215)\n",
      "  CINEMAXIN vs EVEREADY (Corr: -0.7913)\n",
      "  IBSEC vs WELCORP (Corr: -0.7725)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 3. PAIRWISE CORRELATION & TOP 5 NEGATIVE PAIRS (Revised Logic)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 3. Pairwise Correlation and Top 5 Pairs ---\")\n",
    "\n",
    "corr_symbols = growth_symbols # Use ALL stocks for correlation\n",
    "print(f\"DEBUG: Correlating the full set of {len(corr_symbols)} stocks.\")\n",
    "\n",
    "# Pivot CLOSE prices for the selected stocks (2012 only)\n",
    "df_pivot_prices = df_2012_eq.filter(col(\"SYMBOL\").isin(corr_symbols)) \\\n",
    "    .groupBy(\"trade_date\").pivot(\"SYMBOL\").agg(first(\"CLOSE\")).na.fill(0.0)\n",
    "\n",
    "stock_cols = df_pivot_prices.columns[1:]\n",
    "top5_pairs = []\n",
    "\n",
    "if len(stock_cols) < 2:\n",
    "    print(\"WARNING: Insufficient stocks (less than 2) for pairwise correlation.\")\n",
    "else:\n",
    "    # Convert to RDD of vectors\n",
    "    rdd_vectors = df_pivot_prices.select(stock_cols).rdd.map(lambda row: [float(x) for x in row])\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    try:\n",
    "        corr_matrix = Statistics.corr(rdd_vectors, method=\"pearson\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Correlation calculation failed (possible memory/scale issue). Error: {e}\")\n",
    "        # sys.exit(1) # Consider exiting if the core data step fails\n",
    "\n",
    "    # Extract and sort the unique correlation pairs\n",
    "    all_pairs_corr = []\n",
    "    if 'corr_matrix' in locals():\n",
    "        n = len(stock_cols)\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                corr_val = corr_matrix[i][j]\n",
    "                all_pairs_corr.append((stock_cols[i], stock_cols[j], corr_val))\n",
    "\n",
    "        # Sort the list by correlation value ascending (most negative at the top)\n",
    "        all_pairs_corr.sort(key=lambda x: x[2])\n",
    "        top5_pairs = all_pairs_corr[:5]\n",
    "\n",
    "print(\"A. Top 5 negatively correlated pairs (Q3):\")\n",
    "for p in top5_pairs:\n",
    "    print(f\"  {p[0]} vs {p[1]} (Corr: {p[2]:.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4 & 5. Investment Execution and Assessment (2013) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 434:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.A. Total investment on 2013-01-01 (Portfolio): 351.50\n",
      "5.A. Total value on 2013-12-31 (Portfolio): 177.85\n",
      "5.B. Portfolio value change: -173.65\n",
      "5.C. Portfolio percentage change: -49.40%\n",
      "5.D. Stocks increased in price: ['IBSEC']\n",
      "5.E. Stocks decreased in price: ['SITICABLE', 'DBREALTY', 'WELCORP']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 4 & 5. INVESTMENT EXECUTION AND ASSESSMENT (2013)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 4 & 5. Investment Execution and Assessment (2013) ---\")\n",
    "\n",
    "df_2013_eq = df.filter((col(\"trade_year\") == 2013) & (col(\"SERIES\") == 'EQ'))\n",
    "df_2013_eq.cache()\n",
    "\n",
    "# Find first and last trading dates in 2013\n",
    "date_range_2013 = df_2013_eq.agg(min(\"trade_date\").alias(\"start\"), max(\"trade_date\").alias(\"end\")).collect()[0]\n",
    "start_date = date_range_2013[\"start\"]\n",
    "end_date = date_range_2013[\"end\"]\n",
    "\n",
    "# Tally the units to buy for the negatively correlated portfolio\n",
    "units_tally = {}\n",
    "for stock1, stock2, corr in top5_pairs:\n",
    "    units_tally[stock1] = units_tally.get(stock1, 0) + 1\n",
    "    units_tally[stock2] = units_tally.get(stock2, 0) + 1\n",
    "\n",
    "# Get prices on start_date (2013)\n",
    "prices_start_map = df_2013_eq.filter(col(\"trade_date\") == start_date) \\\n",
    "                             .select(\"SYMBOL\", \"CLOSE\").rdd.collectAsMap()\n",
    "\n",
    "# Calculate Initial Investment (4.A)\n",
    "total_investment = builtins.sum(prices_start_map.get(stock, 0.0) * units\n",
    "                                for stock, units in units_tally.items())\n",
    "\n",
    "print(f\"4.A. Total investment on {start_date} (Portfolio): {total_investment:,.2f}\")\n",
    "\n",
    "# Get prices on end_date (2013)\n",
    "prices_end_map = df_2013_eq.filter(col(\"trade_date\") == end_date) \\\n",
    "                           .select(\"SYMBOL\", \"CLOSE\").rdd.collectAsMap()\n",
    "\n",
    "# Calculate Final Value (5.A)\n",
    "final_value = 0.0\n",
    "increased_stocks = []\n",
    "decreased_stocks = []\n",
    "\n",
    "for stock, units in units_tally.items():\n",
    "    p_start = prices_start_map.get(stock, 0.0)\n",
    "    p_end = prices_end_map.get(stock, 0.0)\n",
    "\n",
    "    if p_start > 0 and p_end > 0:\n",
    "        final_value += p_end * units\n",
    "        if p_end > p_start:\n",
    "            increased_stocks.append(stock)\n",
    "        elif p_end < p_start:\n",
    "            decreased_stocks.append(stock)\n",
    "\n",
    "# 5. Metrics\n",
    "portfolio_change = final_value - total_investment\n",
    "percentage_change = (portfolio_change / total_investment) * 100 if total_investment > 0 else 0\n",
    "\n",
    "print(f\"5.A. Total value on {end_date} (Portfolio): {final_value:,.2f}\")\n",
    "print(f\"5.B. Portfolio value change: {portfolio_change:,.2f}\")\n",
    "print(f\"5.C. Portfolio percentage change: {percentage_change:,.2f}%\")\n",
    "print(f\"5.D. Stocks increased in price: {increased_stocks}\")\n",
    "print(f\"5.E. Stocks decreased in price: {decreased_stocks}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. Market Comparison (Top 25 Growth Stocks) ---\n",
      "6.A. Top 5 highly traded, high percentage growth stocks (2012): ['MCDOWELL-N', 'JETAIRWAYS', 'WWIL', 'DBREALTY', 'DISHMAN']\n",
      "6.B. Total initial value (Market Portfolio): 6,082.00\n",
      "6.C. Total final value (Market Portfolio): 5,733.70\n",
      "6.D. Market Portfolio percentage change: -5.73%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[SYMBOL: string, SERIES: string, OPEN: double, HIGH: double, LOW: double, CLOSE: double, LAST: double, PREVCLOSE: double, TOTTRDQTY: bigint, TOTTRDVAL: double, TIMESTAMP: string, ADDNL: string, trade_date: date, trade_year: int]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 6. MARKET COMPARISON (Top 25 Growth Stocks)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 6. Market Comparison (Top 25 Growth Stocks) ---\")\n",
    "\n",
    "# Reuse the growth calculation DataFrame (df_growth_final) but calculate the ratio explicitly\n",
    "df_all_2012_growth_ratio = df_growth_final.withColumn(\n",
    "    \"growth_pct_ratio\", \n",
    "    (col(\"end_price\") - col(\"start_price\")) / col(\"start_price\")\n",
    ")\n",
    "\n",
    "# Select the Top 25 stocks based on 2012 growth ratio\n",
    "df_top25_market = df_all_2012_growth_ratio.orderBy(col(\"growth_pct_ratio\").desc()).limit(25)\n",
    "market_symbols = [row.SYMBOL for row in df_top25_market.select(\"SYMBOL\").collect()]\n",
    "\n",
    "# 6.A: First 5 stocks in the list\n",
    "top5_market_growth = [row.SYMBOL for row in df_top25_market.select(\"SYMBOL\").take(5)]\n",
    "print(f\"6.A. Top 5 highly traded, high percentage growth stocks (2012): {top5_market_growth}\")\n",
    "\n",
    "# Calculate Market Portfolio values (1 unit of each of the 25 stocks)\n",
    "initial_value_market = builtins.sum(prices_start_map.get(sym, 0.0) for sym in market_symbols)\n",
    "final_value_market   = builtins.sum(prices_end_map.get(sym, 0.0) for sym in market_symbols)\n",
    "\n",
    "pct_change_market = (final_value_market - initial_value_market) / initial_value_market * 100 if initial_value_market > 0 else 0\n",
    "\n",
    "print(f\"6.B. Total initial value (Market Portfolio): {initial_value_market:,.2f}\")\n",
    "print(f\"6.C. Total final value (Market Portfolio): {final_value_market:,.2f}\")\n",
    "print(f\"6.D. Market Portfolio percentage change: {pct_change_market:,.2f}%\")\n",
    "\n",
    "df.unpersist()\n",
    "df_2012_eq.unpersist()\n",
    "df_2013_eq.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The adopted strategy of selecting negatively correlated stock pairs proved to be ineffective for the 2013 investment period. The negatively correlated portfolio experienced a severe loss of −49.40% of its value. In stark contrast, the comparative market portfolio (composed of the Top 25 high-growth stocks from 2012) showed a much more resilient performance, incurring a much smaller loss of only −5.73%. This suggests that while the negative correlation strategy is theoretically sound for reducing volatility, in this specific application, it failed to provide the desired downside protection and resulted in a dramatically poorer return than simply holding a diversified group of high-growth stocks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
